<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust </title>
    <meta name="description" content="Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="//cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="icon" href="img/hri_transparent.png">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
        video {
          height: 100%;
          width: 100%;
          object-fit: cover;
        }
    </style>
</head>
<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                Run-time Observation Interventions </br> Make Vision-Language-Action Models More Visually Robust</br>
            </h2>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <br>
                    <li><a href="https://aasherh.github.io/">Asher J. Hancock</a></li>
                    <li><a href="https://allenzren.github.io/">Allen Z. Ren</a></li>
                    <li><a href="https://mae.princeton.edu/people/faculty/majumdar">Anirudha Majumdar</a></li>
                </ul>
            </div>
        </div>
        

        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="https://irom-lab.princeton.edu/">
                    <image src="img/irom_lab.png" height="35px", width="150px" ></image>
                </a>
            </div>
            <div class="col-md-12 text-center">
                <a href="https://www.princeton.edu/">
                    <image src="img/PU1line.svg" height="55px"></image>
                </a>
            </div>
        </div>
        <div class="row mt-2">
            <h3 class="col-md-12 text-center">
                ICRA 2025 </br> </br>
            </h3>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="https://aasherh.github.io/data/Hancock_Visually_Robust_VLAs.pdf">
                    <image src="img/paper.png" height="110px" width="85px"></image>
                <h4><strong>Paper</strong></h4>
                </a>
            </div>
            <!-- TODO -->
            <div class="col-md-2 text-center">
                <a href="https://github.com/SafeRoboticsLab/KLGame">
                <image src="img/github.png"  height="110px"></image>
                <h4><strong>Code</strong></h4>
                </a>
            </div>
            <!-- For when we have folder ready -->
            <!-- <div class="col-md-2 text-center">
                <a href="https://drive.google.com/drive/folders/13qAaPCHEcx93BTcPwwXSITSm7LKsAohr?usp=sharing">
                <image src="img/database.png"  height="110px"></image>
                <h4><strong>
                    Dataset
                </strong></h4>
                </a>
            </div> -->
        </div>



        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <p style="text-align:center;">
                    <image src="img/anchor_figure.png" width="100%"></image>
                </p>
                <h3 class="mt-4 mb-2">
                    Abstract
                </h3>
                <p class="text-justify">
                    Vision-language-action (VLA) models trained on
                    large-scale internet data and robot demonstrations have the
                    potential to serve as generalist robot policies. However, despite
                    their large-scale training, VLAs are often brittle to taskirrelevant visual details such as distractor objects or background
                    colors. We introduce Bring Your Own VLA (BYOVLA): a
                    run-time intervention scheme that (1) dynamically identifies
                    regions of the input image that the model is sensitive to, and (2)
                    minimally alters task-irrelevant regions to reduce the model's
                    sensitivity using automated image editing tools. Our approach
                    is compatible with any off the shelf VLA without model finetuning or access to the model's weights. Hardware experiments
                    on language-instructed manipulation tasks demonstrate that
                    BYOVLA enables state-of-the-art VLA models to nearly retain
                    their nominal performance in the presence of distractor objects
                    and backgrounds, which otherwise degrade task success rates
                    by up to 40%.
                </p>
                <!-- PUT VIDEO HERE LATER
                <div class="col-md-12">
                    <video id="v0" width="60%" preload="metadata" playsinline controls>
                        <source src="videos/final_compressed.mp4" type="video/mp4">
                    </video>
                </div>
                -->
                <!-- <p style="text-align:center;">
                    <image src="img/fig1_rss_real_scenarios.png" width="75%"></image>
                </p> -->
            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">
                    Contributions
                    <!-- Goal -->
                </h3>
                <p class="text-justify">
                    To this end, we propose Bring Your
                    Own VLA (BYOVLA): a run-time intervention scheme that
                    improves visual generalization of off the shelf VLAs by
                    minimally altering regions in the VLA's visual inputs in order
                    to reduce sensitivity against visual distractors. The key idea
                    is to identify (at run-time) which regions of the visual input
                    the model is sensitive to using a visual sensitivity probe that
                    perturbs different segments of the visual input. BYOVLA
                    queries a vision-language model (VLM) to identify which
                    regions in the environment are task-irrelevant and alters a
                    region using automated image editing tools (e.g., inpainting
                    a distractor object) if the region is task-irrelevant and the
                    VLA is sensitive to it

                    BYOVLA can be applied to any VLA model without finetuning or access to the model's weights. Across multiple
                    language-instructed manipulation tasks and varying distractor
                    objects and backgrounds, BYOVLA improves task success
                    rates by 20-40% compared to the original VLA, while also
                    significantly improving performance relative to baselines that
                    perform run-time interventions (1) without accounting for the
                    model's visual sensitivity or (2) assessing sensitivity via prior
                    image attribution methods
                </p>

                <p style="text-align:center;">
                    <image src="img/gradCAM_figure.png" width="100%"></image>
                </p>

            </div>
        </div>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">
                    Approach
                    <!-- This section needs to be rewritten soon -->
                </h3>
                <p class="text-justify">

                    Our approach is as follows

                </p>
                <p class="text-justify">
                    KLGame allows a robot to incorporate <em>guidance</em> from data-driven rollouts while 
                    performing online game-theoretic planning in closed-loop. This method uses a <em>tunable</em> parameter <span>&lambda;</span> 
                    that modulates behaviors on a spectrum: <span>&lambda;=0</span> gives a deterministic dynamic game (<em>task-optimal</em>), 
                    and <span>&lambda;&rarr;&infin;</span> gives multi-modal behavior cloning. We call this tunability <em>policy blending</em>.
                </p>

                <!--
                <p style="text-align:center;">
                    <image src="img/fig2_master_simplified.png" width="100%"></image>
                </p>
                 -->
                <!-- Put a better overview here -->
                <p class="text-justify">
                    BYOVLA Overview?
                    <!-- TODO Figure highlighting all the experiments maybe?-->
                <p class="text-justify">
                    Experimental results and images, videos, gifs.
                    
                </p>

            </div>
        </div>

<!--        <div class="row justify-content-md-center">-->
<!--            <div class="col-md-10 col-lg-8">-->
<!--                <h3 class="mt-4 mb-2">-->
<!--                    TODO: bimanual videos-->
<!--                </h3>-->
<!--            </div>-->
<!--        </div>-->

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h3 class="mt-4 mb-2">
                    Citation
                </h3>
                <pre><code class="codeblock">
                    <!--@article{lidard2024blending,
                        title={Blending Data-Driven Priors in Dynamic Games},
                        author={Lidard, Justin and Hu, Haimin and Hancock, Asher and Zhang, Zixu and Contreras, Albert Gim{\'o} and Modi, Vikash and
                         DeCastro, Jonathan and Gopinath, Deepak and Rosman, Guy and Leonard, Naomi and Santos, Mar{\'i}a and Fisac, Jaime Fern{\'a}ndez},
                        journal={arXiv preprint arXiv:2402.14174},
                        year={2024}
                      }
                -->
                </code></pre>
            </div>
        </div>

    </div>
</body>
</html>
